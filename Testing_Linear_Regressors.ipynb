{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONr5oRBAPNbWD53yb/JyrO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rose-Kimu/Machine-Learning-Classwork/blob/main/Testing_Linear_Regressors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7WeCQNf9OJk7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import patsy\n",
        "\n",
        "import itertools as it\n",
        "import collections as co\n",
        "import functools as ft\n",
        "import os.path as osp\n",
        "\n",
        "import glob\n",
        "import textwrap\n",
        "\n",
        "# finally, a better idiom for warnings\n",
        "import sklearn, warnings\n",
        "warnings.filterwarnings('ignore',\n",
        "                        category=FutureWarning,\n",
        "                        module='sklearn')\n",
        "\n",
        "warnings.filterwarnings('ignore',\n",
        "                        category=FutureWarning,\n",
        "                        module='tensor*')\n",
        "\n",
        "# if the warnings get overwhelming,\n",
        "# you can re-disable with these original lines:\n",
        "#import warnings\n",
        "#warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# and for the really aggressive warnings:\n",
        "# some warnings are stubborn in the extreme, we don't want\n",
        "# them in the book\n",
        "#def warn(*args, **kwargs):  pass\n",
        "#warnings.warn = warn\n",
        "\n",
        "# config related\n",
        "np.set_printoptions(precision=4,\n",
        "                    suppress=True)\n",
        "pd.options.display.float_format = '{:20,.4f}'.format\n",
        "\n",
        "# there are good reasons *NOT* to do this in any real production code\n",
        "# for our purposes (writing a book with completely reproducable output)\n",
        "# this *is* what we want\n",
        "np.random.seed(42)\n",
        "\n",
        "# default is [6.4, 4.8]  (4:3)\n",
        "mpl.rcParams['figure.figsize'] = [4.0, 3.0]\n",
        "\n",
        "# turn on latex tables\n",
        "pd.set_option('display.latex.repr', True)\n",
        "# monkey-patch for centering Out[] DataFrames\n",
        "def _repr_latex_(self):\n",
        "    return \"{\\centering\\n%s\\n\\medskip}\" % self.to_latex()\n",
        "pd.DataFrame._repr_latex_ = _repr_latex_\n",
        "\n",
        "# only used once\n",
        "markers = it.cycle(['+', '^', 'o', '_', '*', 'd', 'x', 's'])\n",
        "\n",
        "\n",
        "# handy helper for displaying stuff\n",
        "from IPython.display import Image\n",
        "\n",
        "#\n",
        "# sklearn's packaging is very java-esque.  :(\n",
        "#\n",
        "from sklearn import (cluster,\n",
        "                     datasets,\n",
        "                     decomposition,\n",
        "                     discriminant_analysis,\n",
        "                     dummy,\n",
        "                     ensemble,\n",
        "                     feature_selection as ftr_sel,\n",
        "                     linear_model,\n",
        "                     metrics,\n",
        "                     model_selection as skms,\n",
        "                     multiclass as skmulti,\n",
        "                     naive_bayes,\n",
        "                     neighbors,\n",
        "                     pipeline,\n",
        "                     preprocessing as skpre,\n",
        "                     svm,\n",
        "                     tree)\n",
        "\n",
        "\n",
        "# the punch line is to predict for a large grid of data points\n",
        "# http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html\n",
        "def plot_boundary(ax, data, tgt, model, dims, grid_step = .01):\n",
        "    # grab a 2D view of the data and get limits\n",
        "    twoD = data[:, list(dims)]\n",
        "    min_x1, min_x2 = np.min(twoD, axis=0) + 2 * grid_step\n",
        "    max_x1, max_x2 = np.max(twoD, axis=0) - grid_step\n",
        "\n",
        "\n",
        "    # make a grid of points and predict at them\n",
        "    xs, ys = np.mgrid[min_x1:max_x1:grid_step,\n",
        "                      min_x2:max_x2:grid_step]\n",
        "    grid_points = np.c_[xs.ravel(), ys.ravel()]\n",
        "    # warning:  non-cv fit\n",
        "    preds = model.fit(twoD, tgt).predict(grid_points).reshape(xs.shape)\n",
        "\n",
        "    # plot the predictions at the grid points\n",
        "    ax.pcolormesh(xs,ys,preds,shading='auto',cmap=plt.cm.coolwarm)\n",
        "    ax.set_xlim(min_x1, max_x1)#-grid_step)\n",
        "    ax.set_ylim(min_x2, max_x2)#-grid_step)\n",
        "\n",
        "def plot_separator(model, xs, ys, label='', ax=None):\n",
        "    ''' xs, ys are 1-D b/c contour and decision_function\n",
        "        use incompatible packaging '''\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "\n",
        "    xy = np_cartesian_product(xs, ys)\n",
        "    z_shape = (xs.size, ys.size) # verus shape[0]?\n",
        "    zs = model.decision_function(xy).reshape(z_shape)\n",
        "\n",
        "    contours = ax.contour(xs, ys, zs,\n",
        "                          colors='k', levels=[0],\n",
        "                          linestyles=['-'])\n",
        "    fmt = {contours.levels[0] : label}\n",
        "    labels = ax.clabel(contours, fmt=fmt, inline_spacing=10)\n",
        "    [l.set_rotation(-90) for l in labels]\n",
        "\n",
        "def high_school_style(ax):\n",
        "    ' helper to define an axis to look like a typical school plot '\n",
        "    ax.spines['left'].set_position(('data', 0.0))\n",
        "    ax.spines['bottom'].set_position(('data', 0.0))\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "\n",
        "    def make_ticks(lims):\n",
        "        lwr, upr = sorted(lims) #x/ylims can be inverted in mpl\n",
        "        lwr = np.round(lwr).astype('int') # can return np objs\n",
        "        upr = np.round(upr).astype('int')\n",
        "        if lwr * upr < 0:\n",
        "            return list(range(lwr, 0)) + list(range(1,upr+1))\n",
        "        else:\n",
        "            return list(range(lwr, upr+1))\n",
        "\n",
        "    import matplotlib.ticker as ticker\n",
        "    xticks = make_ticks(ax.get_xlim())\n",
        "    yticks = make_ticks(ax.get_ylim())\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.FixedLocator(xticks))\n",
        "    ax.yaxis.set_major_locator(ticker.FixedLocator(yticks))\n",
        "\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "def get_model_name(model):\n",
        "    ' return name of model (class) as a string '\n",
        "    return str(model.__class__).split('.')[-1][:-2]\n",
        "\n",
        "def rdot(w,x):\n",
        "    ' apply np.dot on swapped args '\n",
        "    return np.dot(x,w)\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "class DLDA(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, train_ftrs, train_tgts):\n",
        "        self.uniq_tgts = np.unique(train_tgts)\n",
        "        self.means, self.priors = {}, {}\n",
        "\n",
        "        self.var  = train_ftrs.var(axis=0) # biased\n",
        "        for tgt in self.uniq_tgts:\n",
        "            cases = train_ftrs[train_tgts==tgt]\n",
        "            self.means[tgt]  = cases.mean(axis=0)\n",
        "            self.priors[tgt] = len(cases) / len(train_ftrs)\n",
        "        return self\n",
        "\n",
        "    def predict(self, test_ftrs):\n",
        "        disc = np.empty((test_ftrs.shape[0],\n",
        "                         self.uniq_tgts.shape[0]))\n",
        "        for tgt in self.uniq_tgts:\n",
        "            # technically, the maha_dist is sqrt() of this:\n",
        "            mahalanobis_dists = ((test_ftrs - self.means[tgt])**2 /\n",
        "                                 self.var)\n",
        "            disc[:,tgt] = (-np.sum(mahalanobis_dists, axis=1) +\n",
        "                           2 * np.log(self.priors[tgt]))\n",
        "        return np.argmax(disc,axis=1)\n",
        "\n",
        "\n",
        "def plot_lines_and_projections(axes, lines, points, xs):\n",
        "    data_xs, data_ys = points[:,0], points[:,1]\n",
        "    mean = np.mean(points, axis=0, keepdims=True)\n",
        "    centered_data = points - mean\n",
        "\n",
        "    for (m,b), ax in zip(lines, axes):\n",
        "        mb_line = m*xs + b\n",
        "        v_line = np.array([[1, 1/m if m else 0]])\n",
        "\n",
        "        ax.plot(data_xs, data_ys, 'r.') # uncentered\n",
        "        ax.plot(xs, mb_line, 'y')      # uncentered\n",
        "        ax.plot(*mean.T, 'ko')\n",
        "\n",
        "        # centered data makes the math easier!\n",
        "        # this is length on yellow line from red to blue\n",
        "        # distance from mean to projected point\n",
        "        y_lengths = centered_data.dot(v_line.T) / v_line.dot(v_line.T)\n",
        "        projs = y_lengths.dot(v_line)\n",
        "\n",
        "        # decenter (back to original coordinates)\n",
        "        final = projs + mean\n",
        "        ax.plot(*final.T, 'b.')\n",
        "\n",
        "        # connect points to projections\n",
        "        from matplotlib import collections as mc\n",
        "        proj_lines = mc.LineCollection(zip(points,final))\n",
        "        ax.add_collection(proj_lines)\n",
        "\n",
        "        hypots = zip(points, np.broadcast_to(mean, points.shape))\n",
        "        mean_lines = mc.LineCollection(hypots, linestyles='dashed')\n",
        "        ax.add_collection(mean_lines)\n",
        "\n",
        "# adding an orientation would be nice\n",
        "def sane_quiver(vs, ax=None, colors=None, origin=(0,0)):\n",
        "    '''plot row vectors from origin'''\n",
        "    vs = np.asarray(vs)\n",
        "    assert vs.ndim == 2 and vs.shape[1] == 2  # ensure column vectors\n",
        "    n = vs.shape[0]\n",
        "    if not ax: ax = plt.gca()\n",
        "\n",
        "    orig_x, orig_y = origin\n",
        "    xs = vs.T[0]  # column to rows, row[0] is xs\n",
        "    ys = vs.T[1]\n",
        "\n",
        "    # highly annoying:  quiver doesn't broadcast anymore (?)\n",
        "    orig_x = np.full_like(xs, orig_x)\n",
        "    orig_x = np.full_like(xs, orig_x)\n",
        "\n",
        "    props = {\"angles\":'xy', 'scale':1, 'scale_units':'xy'}\n",
        "    ax.quiver(orig_x, orig_y, xs, ys, color=colors, **props)\n",
        "\n",
        "    ax.set_aspect('equal')\n",
        "    # ax.set_axis_off()\n",
        "    _min, _max = min(vs.min(), 0) -1, max(0, vs.max())+1\n",
        "    ax.set_xlim(_min, _max)\n",
        "    ax.set_ylim(_min, _max)\n",
        "\n",
        "def reweight(examples, weights):\n",
        "    ''' convert weights to counts of examples using approximately two\n",
        "        significant digits of weights.\n",
        "\n",
        "        there are probably a 100 reasons not to do this like this.\n",
        "        top 2:\n",
        "          1.  boosting may require more precise values (or using randomization)\n",
        "              to keep things unbiased\n",
        "          2.  this *really* expands the dataset to a significant degree\n",
        "              (wastes resources)\n",
        "    '''\n",
        "    from math import gcd\n",
        "    from functools import reduce\n",
        "\n",
        "    # who needs repeated the least?\n",
        "    min_wgt = min(weights)\n",
        "    min_replicate = 1 / min_wgt # e.g., .25 -> 4\n",
        "\n",
        "    # compute naive duplication to 2 decimal places\n",
        "    counts = (min_replicate * weights * 100).astype(np.int64)\n",
        "\n",
        "    # trim duplication if we can\n",
        "    our_gcd = reduce(gcd, counts)\n",
        "    counts = counts // our_gcd\n",
        "\n",
        "    # repeat is picky about type\n",
        "    return np.repeat(examples, counts, axis=0)\n",
        "\n",
        "#examples = np.array([1, 10, 20])\n",
        "#weights  = np.array([.25, .33, 1-(.25+.33)])\n",
        "# print(pd.Series(reweight(examples, weights)))\n",
        "\n",
        "def enumerate_outer(outer_seq):\n",
        "    '''repeat the outer idx based on len of inner'''\n",
        "    return np.repeat(*zip(*enumerate(map(len, outer_seq))))\n",
        "\n",
        "def np_array_fromiter(itr, shape, dtype=np.float64):\n",
        "    ''' helper since np.fromiter only does 1D'''\n",
        "    arr = np.empty(shape, dtype=dtype)\n",
        "    for idx, itm in enumerate(itr):\n",
        "        arr[idx] = itm\n",
        "    return arr\n",
        "\n",
        "# how do you figure out arcane code?\n",
        "# work inside out, small inputs, pay attention to datatypes.\n",
        "# try outter and righter calls with simpler inputs\n",
        "# read docs *in conjunction with* experiments\n",
        "# [the docs rarely make sense - to me - in the abstract until I try\n",
        "#  examples while reading them]\n",
        "\n",
        "# the difference with a \"raw\" np.meshgrid call is we stack these up in\n",
        "# two columns of results (i.e., we make a table out of the pair arrays)\n",
        "def np_cartesian_product(*arrays):\n",
        "    ''' some numpy kung-fu to produce all\n",
        "        possible combinations of input arrays '''\n",
        "    ndim = len(arrays)\n",
        "    return np.stack(np.meshgrid(*arrays), axis=-1).reshape(-1, ndim)\n",
        "\n",
        "# replacement for tsplot is happiest with\n",
        "# \"tidy\" data\n",
        "# tidying the numpy array is a bit of a pain\n",
        "# xarray is designed to do this \"natively\" but\n",
        "# i don't want to introduce that dependency\n",
        "# [seems like there could be a better broadcasting\n",
        "#  solution to this]\n",
        "def sk_graph_to_tidy(train_test_scores, # y values\n",
        "                     eval_points,       # x values\n",
        "                     eval_label,        # x column name\n",
        "                     num_folds):        # could be inferred\n",
        "    train_scores, test_scores = train_test_scores\n",
        "    # humph, didn't know np_cartesian was order sensitive\n",
        "    labels = np_cartesian_product(eval_points,\n",
        "                                  [0,1], # surrogates for train/test\n",
        "                                  np.arange(num_folds))\n",
        "    score = np.concatenate([train_scores.flatten(),\n",
        "                            test_scores.flatten()], axis=0)\n",
        "\n",
        "    df = pd.DataFrame.from_records(labels)\n",
        "    df.columns = [eval_label, 'set', 'fold']\n",
        "    df.set = df.set.replace({0:'Train', 1:'Test'})\n",
        "    df['score'] = score\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stand-alone code\n",
        "from sklearn import (datasets, neighbors,model_selection as skms,linear_model, metrics)\n",
        "\n",
        "diabetes = datasets.load_diabetes()\n",
        "tts = skms.train_test_split(diabetes.data, diabetes.target,test_size=.25)\n",
        "\n",
        "(diabetes_train, diabetes_test,\n",
        "diabetes_train_tgt, diabetes_test_tgt) = tts\n",
        "\n",
        "models = {'kNN': neighbors.KNeighborsRegressor(n_neighbors=5),\n",
        "'linreg' : linear_model.LinearRegression()}\n",
        "\n",
        "\n",
        "for name, model in models.items():\n",
        "  fit = model.fit(diabetes_train, diabetes_train_tgt)\n",
        "  preds = fit.predict(diabetes_test)\n",
        "  score = np.sqrt(metrics.mean_squared_error(diabetes_test_tgt, preds))\n",
        "  print(\"{:>6s} : {:0.2f}\".format(name,score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dlqqzGKOfGy",
        "outputId": "7920478d-5803-4e36-82e1-2b63a1dc3cf0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   kNN : 62.46\n",
            "linreg : 57.38\n"
          ]
        }
      ]
    }
  ]
}